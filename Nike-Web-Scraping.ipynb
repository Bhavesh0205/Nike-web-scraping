{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74017f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "# import io\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create Functions to load this data\n",
    "# Function for Name\n",
    "def load_name(soup):\n",
    "    Name = soup.find('h1',attrs ={'id':'pdp_product_title'}).text.strip()\n",
    "    return Name\n",
    "\n",
    "# Function for description\n",
    "def load_description(soup):\n",
    "    Description = soup.find('div', attrs = {'class':'description-preview body-2 css-1pbvugb'}).text.strip()\n",
    "    return Description\n",
    "\n",
    "# Function for Price\n",
    "def load_price(soup):\n",
    "    try:\n",
    "        Price = soup.find('div',attrs ={'class':'product-price css-11s12ax is--current-price css-tpaepq'}).text.strip()\n",
    "    except:\n",
    "        Price = ''\n",
    "    return Price\n",
    "Nike = {'Name':[],'Price':[],'Description':[],'Link':[]}\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    Nike_URL = 'https://www.nike.com/w/mens-shoes-nik1zy7ok'\n",
    "    HEADERS =({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "          'Accept-Language':'en-US, en; q =0.5'})\n",
    "    # Request connection to wepage\n",
    "    Nike_webpage = requests.get(Nike_URL, headers = HEADERS)\n",
    "    Nike_webpage\n",
    "    # Convert to HTML using beautifyul soup\n",
    "    Nike_soup = BeautifulSoup(Nike_webpage.content, 'html.parser')\n",
    "    # Get all links on the page\n",
    "    links = Nike_soup.find_all('a', attrs={'class':'product-card__link-overlay'})\n",
    "    # Loop through all the data to get just hyperlinks\n",
    "    links_list = []\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "    for link in links_list:\n",
    "#     Send Connection request to all links\n",
    "        Nike_Webpages = requests.get(link, headers = HEADERS)\n",
    "#     convert byte to html \n",
    "        Nike_Soup = BeautifulSoup(Nike_Webpages.content, 'html.parser')    \n",
    "    \n",
    "#     Get all releveant information using functions\n",
    "        Nike['Name'].append(load_name(Nike_Soup))\n",
    "        Nike['Description'].append(load_description(Nike_Soup))\n",
    "        Nike['Price'].append(load_price(Nike_Soup))\n",
    "        Nike['Link'].append(link)\n",
    "    # print(Nike)\n",
    "    Nike_df = pd.DataFrame.from_dict(Nike)\n",
    "       \n",
    "    bucket = 'nik-web-scrapping-project' # already created on S3\n",
    "    csv_buffer = StringIO()\n",
    "    Nike_df.to_csv(csv_buffer, index =False)\n",
    "    s3_resource = boto3.client('s3')\n",
    "    filename = \"Nike_df\" + str(datetime.now()) + \".csv\"\n",
    "    key =  \"Nike_data/\"  + filename\n",
    "    # s3_resource.Object(bucket, 'Nike_df').put(Body=csv_buffer.getvalue())\n",
    "    s3_resource.put_object(Bucket = bucket, Key = key, Body=csv_buffer.getvalue())\n",
    "   \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
